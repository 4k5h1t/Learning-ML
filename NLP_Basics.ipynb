{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+aDjCXJEpPnVCyjxhGHyN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/4k5h1t/Learning-ML/blob/main/NLP_Basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MzXtrkIG71Y_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPH0UIJY70yH",
        "outputId": "a34822fb-ad80-482e-d01e-04194eaecb61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nltk==3.5\n",
            "  Downloading nltk-3.5.zip (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 4.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk==3.5) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk==3.5) (1.2.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from nltk==3.5) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk==3.5) (4.64.1)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434691 sha256=a1172e11f46737cf75c7e48bea9f1c628dfa1dc46f6b5cd35c93a9e90c33a819\n",
            "  Stored in directory: /root/.cache/pip/wheels/ff/d5/7b/f1fb4e1e1603b2f01c2424dd60fbcc50c12ef918bafc44b155\n",
            "Successfully built nltk\n",
            "Installing collected packages: nltk\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.7\n",
            "    Uninstalling nltk-3.7:\n",
            "      Successfully uninstalled nltk-3.7\n",
            "Successfully installed nltk-3.5\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk==3.5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "example_string = \"\"\"Muad'Dib learned rapidly because his first training was in how to learn.\n",
        "                    And the first lesson of all was the basic trust that he could learn.\n",
        "                    It's shocking to find how many people do not believe they can learn, and how many more believe learning to be difficult.\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rT7jbv7Y8Q5W",
        "outputId": "27421ea2-aaa6-478f-88f1-56bc4bee2361"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizing \n",
        "Splitting into Logical Units"
      ],
      "metadata": {
        "id": "std2n_VL0QdB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokenize(example_string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Te0dWAPJ8jX7",
        "outputId": "79073af2-dd1e-4b4e-b1ac-61b5001b85bf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Muad'Dib learned rapidly because his first training was in how to learn.\",\n",
              " 'And the first lesson of all was the basic trust that he could learn.',\n",
              " \"It's shocking to find how many people do not believe they can learn, and how many more believe learning to be difficult.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize(example_string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6uHnwFR-amq",
        "outputId": "13d0e32f-d4c0-4f3d-bd7e-1bb58dce2aa2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Muad'Dib\",\n",
              " 'learned',\n",
              " 'rapidly',\n",
              " 'because',\n",
              " 'his',\n",
              " 'first',\n",
              " 'training',\n",
              " 'was',\n",
              " 'in',\n",
              " 'how',\n",
              " 'to',\n",
              " 'learn',\n",
              " '.',\n",
              " 'And',\n",
              " 'the',\n",
              " 'first',\n",
              " 'lesson',\n",
              " 'of',\n",
              " 'all',\n",
              " 'was',\n",
              " 'the',\n",
              " 'basic',\n",
              " 'trust',\n",
              " 'that',\n",
              " 'he',\n",
              " 'could',\n",
              " 'learn',\n",
              " '.',\n",
              " 'It',\n",
              " \"'s\",\n",
              " 'shocking',\n",
              " 'to',\n",
              " 'find',\n",
              " 'how',\n",
              " 'many',\n",
              " 'people',\n",
              " 'do',\n",
              " 'not',\n",
              " 'believe',\n",
              " 'they',\n",
              " 'can',\n",
              " 'learn',\n",
              " ',',\n",
              " 'and',\n",
              " 'how',\n",
              " 'many',\n",
              " 'more',\n",
              " 'believe',\n",
              " 'learning',\n",
              " 'to',\n",
              " 'be',\n",
              " 'difficult',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filtering Stop Words\n",
        "Stop words are words that you want to ignore, so you filter them out of your text when you’re processing it. Very common words like 'in', 'is', and 'an' are often used as stop words since they don’t add a lot of meaning to a text in and of themselves."
      ],
      "metadata": {
        "id": "Gkf70L7l0szL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTV_T2fs-ckx",
        "outputId": "604fdefe-d6fd-4321-ebd0-285243adf0f7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "worf_quote = \"Sir, I protest. I am not a merry man!\"\n",
        "words_in_quote = word_tokenize(worf_quote)\n",
        "words_in_quote"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z52PVCBphxrt",
        "outputId": "e4ac64ff-fc22-4683-8357-ef0ed1811f75"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Sir', ',', 'I', 'protest', '.', 'I', 'am', 'not', 'a', 'merry', 'man', '!']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words(\"english\"))\n",
        "filtered_list = [word for word in words_in_quote if word.casefold() not in stop_words]\n",
        "filtered_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0R4Adloij9a",
        "outputId": "174b98e7-9c3b-47f8-a8e7-1d0528072035"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Sir', ',', 'protest', '.', 'merry', 'man', '!']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is not a great form of filtering words...\n",
        "Many important words were lost, others that were not needed were kept"
      ],
      "metadata": {
        "id": "lTZjSFjh02AN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming\n",
        "\n",
        "Stemming is a text processing task in which you reduce words to their root, which is the core part of a word. For example, the words “helping” and “helper” share the root “help.” Stemming allows you to zero in on the basic meaning of a word rather than all the details of how it’s being used. NLTK has more than one stemmer"
      ],
      "metadata": {
        "id": "PtHMK8l11HV9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the Porter Stemmer"
      ],
      "metadata": {
        "id": "A3XqmNbq1ZRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "string_for_stemming = \"The crew of the USS Discovery discovered many discoveries. Discovering is what explorers do.\""
      ],
      "metadata": {
        "id": "1Kqi_o-5ycqO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = word_tokenize(string_for_stemming)\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzSCGEecy1v1",
        "outputId": "c3756741-65a0-4ab0-c8bc-2aa8d3c50865"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'crew',\n",
              " 'of',\n",
              " 'the',\n",
              " 'USS',\n",
              " 'Discovery',\n",
              " 'discovered',\n",
              " 'many',\n",
              " 'discoveries',\n",
              " '.',\n",
              " 'Discovering',\n",
              " 'is',\n",
              " 'what',\n",
              " 'explorers',\n",
              " 'do',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmed_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ox8vBZN0FjZ",
        "outputId": "ec935eaf-06ec-4575-ab1e-0b14f40b2167"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the',\n",
              " 'crew',\n",
              " 'of',\n",
              " 'the',\n",
              " 'uss',\n",
              " 'discoveri',\n",
              " 'discov',\n",
              " 'mani',\n",
              " 'discoveri',\n",
              " '.',\n",
              " 'discov',\n",
              " 'is',\n",
              " 'what',\n",
              " 'explor',\n",
              " 'do',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clearly this is not a great Stemmer as many words have been garbled and they haven't been grouped properly\n",
        "\n",
        "Check out Snowball Stemmer"
      ],
      "metadata": {
        "id": "Eb4lVZ-31cpi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tfox7jeX2uEF"
      }
    }
  ]
}